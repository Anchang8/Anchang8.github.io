---
---

@article{An2023CaPTURe,
  author    = {Chang-Hyeon An and Hyun-Chul Choi},
  title     = {CaPTURe: Cartoon Pose Transfer Using Reverse Attention},
  abstract  = {Most of the previous pose transfer methods require additional input data such as joint keypoints of the target pose extracted by a pre-trained network in a human domain. However, in the fields where cartoon characters are used, such as animation or webtoons, the body proportions and structures of the characters differ from those of actual people, so it is improper to extract additional data using the pre-trained network in the human domain. Even if the network is newly trained in the cartoon domain, expensive data labeling is necessary. As a result, most of the previous pose transfer methods are unsuitable to utilize in the cartoon domain. To address these issues, we propose a Cartoon Pose Transfer network (CaPTURe) that can successfully perform pose transfer in the cartoon domain with only target images and no other input data. Our proposed CaPTURe receives identity and pose source images and extracts information from each source image using effectively designed encoders for cartoon pose transfer. Then the network decodes the identity and pose information with an attention module that makes the network generate the desired conditions more accurately. Here, we adopt the attention mechanism to properly generate the desired identity. On the other hand, we utilize reverse attention to generate the desired pose more precisely, which excludes the identity information of the pose feature and allows us to use only the pose information. Consequently, through comparative experiments using a cartoon domain dataset, CaPTURe shows quantitative results higher by 4.7% in L1 Distance, 0.1% in SSIM, and 0.8% in LPIPS, as well as being more qualitatively superior to the previous state-of-the-art methods. Moreover, we demonstrate that CaPTURe is capable of achieving effective pose transfer not just in the cartoon domains, but also in the human domains, as evidenced by our experiments on a human domain dataset.},
  year      = {2023},
  month     = {May},
  journal={Neurocomputing},
  selected={true},
  additional_info={Minor Revision},
  img_path={assets/img/CaPTURe.png}
}

@article{an2022part,
  title={Part Affinity Fields and CoordConv for Detecting Landmarks of Lumbar Vertebrae and Sacrum in X-ray Images},
  author={An, Chang-Hyeon and Lee, Jeong-Sik and Jang, Jun-Su and Choi, Hyun-Chul},
  journal={Sensors},
  volume={22},
  number={22},
  pages={8628},
  year={2022},
  publisher={MDPI},
  selected={true},
  website={https://www.mdpi.com/1424-8220/22/22/8628},
  img_path={assets/img/sensors_overview.png},
  abstract={With the prevalence of degenerative diseases due to the increase in the aging population, we have encountered many spine-related disorders. Since the spine is a crucial part of the body, fast and accurate diagnosis is critically important. Generally, clinicians use X-ray images to diagnose the spine, but X-ray images are commonly occluded by the shadows of some bones, making it hard to identify the whole spine. Therefore, recently, various deep-learning-based spinal X-ray image analysis approaches have been proposed to help diagnose the spine. However, these approaches did not consider the characteristics of frequent occlusion in the X-ray image and the properties of the vertebra shape. Therefore, based on the X-ray image properties and vertebra shape, we present a novel landmark detection network specialized in lumbar X-ray images. The proposed network consists of two stages: The first step detects the centers of the lumbar vertebrae and the upper end plate of the first sacral vertebra (S1), and the second step detects the four corner points of each lumbar vertebra and two corner points of S1 from the image obtained in the first step. We used random spine cutout augmentation in the first step to robustify the network against the commonly obscured X-ray images. Furthermore, in the second step, we used CoordConv to make the network recognize the location distribution of landmarks and part affinity fields to understand the morphological features of the vertebrae, resulting in more accurate landmark detection. The proposed network was evaluated using 304 X-ray images, and it achieved 98.02% accuracy in center detection and 8.34% relative distance error in corner detection. This indicates that our network can detect spinal landmarks reliably enough to support radiologists in analyzing the lumbar X-ray images.}
}