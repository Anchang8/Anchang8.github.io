---
---

@article{An2023CaPTURe,
  author    = {Chang-Hyeon An and Hyun-Chul Choi},
  title     = {CaPTURe: Cartoon Pose Transfer Using Reverse Attention},
  abstract  = {Most of the previous pose transfer methods require additional input data such as joint keypoints of the target pose extracted by a pre-trained network in a human domain. However, in the fields where cartoon characters are used, such as animation or webtoons, the body proportions and structures of the characters differ from those of actual people, so it is improper to extract additional data using the pre-trained network in the human domain. Even if the network is newly trained in the cartoon domain, expensive data labeling is necessary. As a result, most of the previous pose transfer methods are unsuitable to utilize in the cartoon domain. To address these issues, we propose a Cartoon Pose Transfer network (CaPTURe) that can successfully perform pose transfer in the cartoon domain with only target images and no other input data. Our proposed CaPTURe receives identity and pose source images and extracts information from each source image using effectively designed encoders for cartoon pose transfer. Then the network decodes the identity and pose information with an attention module that makes the network generate the desired conditions more accurately. Here, we adopt the attention mechanism to properly generate the desired identity. On the other hand, we utilize reverse attention to generate the desired pose more precisely, which excludes the identity information of the pose feature and allows us to use only the pose information. Consequently, through comparative experiments using a cartoon domain dataset, CaPTURe shows quantitative results higher by 4.7% in L1 Distance, 0.1% in SSIM, and 0.8% in LPIPS, as well as being more qualitatively superior to the previous state-of-the-art methods. Moreover, we demonstrate that CaPTURe is capable of achieving effective pose transfer not just in the cartoon domains, but also in the human domains, as evidenced by our experiments on a human domain dataset.},
  year      = {2023},
  month     = {Oct.},
  journal={Neurocomputing},
  website={https://anchang8.github.io/CaPTURe_Demo},
  pdf={https://www.sciencedirect.com/science/article/abs/pii/S0925231223007427?via%3Dihub},
  abstract={Most of the previous pose transfer methods require additional input data such as joint keypoints of the target pose extracted by a pre-trained network in a human domain. However, in the fields where cartoon characters are used, such as animation or webtoons, the body proportions and structures of the characters often deviate from those of real-life humans. Therefore, it is not appropriate to utilize a pre-trained network designed for the human domain to extract additional data from these cartoon characters. Even if the network is newly trained in the cartoon domain, expensive data labeling is necessary. As a result, most of the previous pose transfer methods are not suitable for application in the cartoon domain. To address these issues, we propose a cartoon pose transfer network named CaPTURe that can successfully perform pose transfer in the cartoon domain with only target images and no other input data. Here, we incorporate the attention mechanism to accurately generate the desired identity. Additionally, we employ reverse attention to enhance the precision of generating the target pose. This approach eliminates the influence of identity information in the pose feature, enabling our network to focus solely on utilizing pose information. Consequently, through comparative experiments using a cartoon domain dataset, CaPTURe shows significant improvements in quantitative results. Specifically, it achieves a 4.7% reduction in L1 Distance, a 0.1% improvement in SSIM, and a 0.8% enhancement in LPIPS. Moreover, CaPTURe exhibits superior qualitative performance than the previous state-of-the-art methods. In addition, we demonstrate that CaPTURe is capable of achieving effective pose transfer not just in the cartoon domains, but also in the human domains, as evidenced by our experiments on a human domain dataset.},
  selected={true},
  img_path={assets/img/CaPTURe.png}
}

@article{yoon2023semicon,
  author    = {Yun*, Hyeok and An*, Chang-Hyeon and Jang, Hyundong and Cho, Kyeongrae and Lee, Jeong-Sik and Eom, Seungjoon and Kim, Choong-Ki and Yoo, Min-Soo and Choi, Hyun-Chul and Baek, Rock-Hyun},
  title     = {Accurate Prediction and Reliable Parameter Optimization of Neural Network for Semiconductor Process Monitoring and Technology Development},
  volume={},
  abstract  = {Herein, novel neural network (NN) methods that improve prediction accuracy and reduce output variance of the optimized input in the gradient method for cross-sectional data are proposed, and the variability evaluation approach of optimized inputs in the semiconductor process is suggested. Specifically, electrical parameter measurements (EPMs) and power-delay product of industrial high-k metal gate DRAM peripheral 29-stage ring oscillator circuits, including NMOS, PMOS, and interconnects, are focused on. The proposed methods find an optimized input to achieve a lower NN output variance in the gradient descent than one multilayer perceptron (MLP) and mean ensemble of MLPs even when considering the variabilities of the devices and interconnects. The local optima problem of one MLP is resolved by utilizing multiple MLPs trained with different train/validation data, their trimmed mean, and an additional learnable layer. Moreover, adding the learnable layer secures versatility for various parametric datasets. The methods improve the prediction accuracy (R2) by 5.6–15.6% in sparse data space compared to one MLP and the mean ensemble, decrease the NN output variance of the optimized input by 73.0–81.6% compared to one MLP and the mean ensemble, and are successfully verified by implementing it on EPMs of 3977 test patterns of 314 wafers and 16 lots.},
  year      = {2023},
  month     = {July},
  journal={Advanced Intelligent Systems},
  selected={true},
  img_path={assets/img/trim_mean.jpg},
  pdf={https://onlinelibrary.wiley.com/doi/10.1002/aisy.202300089},
  equal_contrib={true}
}

@article{an2022part,
  title={Part Affinity Fields and CoordConv for Detecting Landmarks of Lumbar Vertebrae and Sacrum in X-ray Images},
  author={An, Chang-Hyeon and Lee, Jeong-Sik and Jang, Jun-Su and Choi, Hyun-Chul},
  journal={Sensors},
  volume={22},
  number={22},
  pages={8628},
  year={2022},
  month     = {Nov.},
  publisher={MDPI},
  selected={true},
  pdf={https://www.mdpi.com/1424-8220/22/22/8628},
  img_path={assets/img/sensors_overview.png},
  abstract={With the prevalence of degenerative diseases due to the increase in the aging population, we have encountered many spine-related disorders. Since the spine is a crucial part of the body, fast and accurate diagnosis is critically important. Generally, clinicians use X-ray images to diagnose the spine, but X-ray images are commonly occluded by the shadows of some bones, making it hard to identify the whole spine. Therefore, recently, various deep-learning-based spinal X-ray image analysis approaches have been proposed to help diagnose the spine. However, these approaches did not consider the characteristics of frequent occlusion in the X-ray image and the properties of the vertebra shape. Therefore, based on the X-ray image properties and vertebra shape, we present a novel landmark detection network specialized in lumbar X-ray images. The proposed network consists of two stages: The first step detects the centers of the lumbar vertebrae and the upper end plate of the first sacral vertebra (S1), and the second step detects the four corner points of each lumbar vertebra and two corner points of S1 from the image obtained in the first step. We used random spine cutout augmentation in the first step to robustify the network against the commonly obscured X-ray images. Furthermore, in the second step, we used CoordConv to make the network recognize the location distribution of landmarks and part affinity fields to understand the morphological features of the vertebrae, resulting in more accurate landmark detection. The proposed network was evaluated using 304 X-ray images, and it achieved 98.02% accuracy in center detection and 8.34% relative distance error in corner detection. This indicates that our network can detect spinal landmarks reliably enough to support radiologists in analyzing the lumbar X-ray images.}
}

@article{ullah2022review,
  title={A Review of Multi-Modal Learning from the Text-Guided Visual Processing Viewpoint},
  author={Ullah, Ubaid and Lee, Jeong-Sik and An, Chang-Hyeon and Lee, Hyeonjin and Park, Su-Yeong and Baek, Rock-Hyun and Choi, Hyun-Chul},
  journal={Sensors},
  volume={22},
  number={18},
  pages={6816},
  year={2022},
  month     = {Sep.},
  publisher={MDPI},
  pdf={https://www.mdpi.com/1424-8220/22/18/6816},
  selected={true},
  img_path={assets/img/sensors_review_overview.png},
  abstract={For decades, co-relating different data domains to attain the maximum potential of machines has driven research, especially in neural networks. Similarly, text and visual data (images and videos) are two distinct data domains with extensive research in the past. Recently, using natural language to process 2D or 3D images and videos with the immense power of neural nets has witnessed a promising future. Despite the diverse range of remarkable work in this field, notably in the past few years, rapid improvements have also solved future challenges for researchers. Moreover, the connection between these two domains is mainly subjected to GAN, thus limiting the horizons of this field. This review analyzes Text-to-Image (T2I) synthesis as a broader picture, Text-guided Visual-output (T2Vo), with the primary goal being to highlight the gaps by proposing a more comprehensive taxonomy. We broadly categorize text-guided visual output into three main divisions and meaningful subdivisions by critically examining an extensive body of literature from top-tier computer vision venues and closely related fields, such as machine learning and human–computer interaction, aiming at state-of-the-art models with a comparative analysis. This study successively follows previous surveys on T2I, adding value by analogously evaluating the diverse range of existing methods, including different generative models, several types of visual output, critical examination of various approaches, and highlighting the shortcomings, suggesting the future direction of research.}
}