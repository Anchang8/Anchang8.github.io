---
---

@article{An2023CaPTURe,
  author    = {Chang-Hyeon An and Hyun-Chul Choi},
  title     = {CaPTURe: Cartoon Pose Transfer Using Reverse Attention},
  abstract  = {Most of the previous pose transfer methods require additional input data such as joint keypoints of the target pose extracted by a pre-trained network in a human domain. However, in the fields where cartoon characters are used, such as animation or webtoons, the body proportions and structures of the characters differ from those of actual people, so it is improper to extract additional data using the pre-trained network in the human domain. Even if the network is newly trained in the cartoon domain, expensive data labeling is necessary. As a result, most of the previous pose transfer methods are unsuitable to utilize in the cartoon domain. To address these issues, we propose a Cartoon Pose Transfer network (CaPTURe) that can successfully perform pose transfer in the cartoon domain with only target images and no other input data. Our proposed CaPTURe receives identity and pose source images and extracts information from each source image using effectively designed encoders for cartoon pose transfer. Then the network decodes the identity and pose information with an attention module that makes the network generate the desired conditions more accurately. Here, we adopt the attention mechanism to properly generate the desired identity. On the other hand, we utilize reverse attention to generate the desired pose more precisely, which excludes the identity information of the pose feature and allows us to use only the pose information. Consequently, through comparative experiments using a cartoon domain dataset, CaPTURe shows quantitative results higher by 4.7% in L1 Distance, 0.1% in SSIM, and 0.8% in LPIPS, as well as being more qualitatively superior to the previous state-of-the-art methods. Moreover, we demonstrate that CaPTURe is capable of achieving effective pose transfer not just in the cartoon domains, but also in the human domains, as evidenced by our experiments on a human domain dataset.},
  year      = {2023},
  month     = {May},
  journal={Minor Revision},
  website={https://anchang8.github.io/CaPTURe_Demo},
  selected={true},
  img_path={assets/img/CaPTURe.png}
}

@article{an2022part,
  title={Part Affinity Fields and CoordConv for Detecting Landmarks of Lumbar Vertebrae and Sacrum in X-ray Images},
  author={An, Chang-Hyeon and Lee, Jeong-Sik and Jang, Jun-Su and Choi, Hyun-Chul},
  journal={Sensors},
  volume={22},
  number={22},
  pages={8628},
  year={2022},
  publisher={MDPI},
  selected={true},
  website={https://www.mdpi.com/1424-8220/22/22/8628},
  img_path={assets/img/sensors_overview.png},
  abstract={With the prevalence of degenerative diseases due to the increase in the aging population, we have encountered many spine-related disorders. Since the spine is a crucial part of the body, fast and accurate diagnosis is critically important. Generally, clinicians use X-ray images to diagnose the spine, but X-ray images are commonly occluded by the shadows of some bones, making it hard to identify the whole spine. Therefore, recently, various deep-learning-based spinal X-ray image analysis approaches have been proposed to help diagnose the spine. However, these approaches did not consider the characteristics of frequent occlusion in the X-ray image and the properties of the vertebra shape. Therefore, based on the X-ray image properties and vertebra shape, we present a novel landmark detection network specialized in lumbar X-ray images. The proposed network consists of two stages: The first step detects the centers of the lumbar vertebrae and the upper end plate of the first sacral vertebra (S1), and the second step detects the four corner points of each lumbar vertebra and two corner points of S1 from the image obtained in the first step. We used random spine cutout augmentation in the first step to robustify the network against the commonly obscured X-ray images. Furthermore, in the second step, we used CoordConv to make the network recognize the location distribution of landmarks and part affinity fields to understand the morphological features of the vertebrae, resulting in more accurate landmark detection. The proposed network was evaluated using 304 X-ray images, and it achieved 98.02% accuracy in center detection and 8.34% relative distance error in corner detection. This indicates that our network can detect spinal landmarks reliably enough to support radiologists in analyzing the lumbar X-ray images.}
}

@article{ullah2022review,
  title={A Review of Multi-Modal Learning from the Text-Guided Visual Processing Viewpoint},
  author={Ullah, Ubaid and Lee, Jeong-Sik and An, Chang-Hyeon and Lee, Hyeonjin and Park, Su-Yeong and Baek, Rock-Hyun and Choi, Hyun-Chul},
  journal={Sensors},
  volume={22},
  number={18},
  pages={6816},
  year={2022},
  publisher={MDPI},
  website={https://www.mdpi.com/1424-8220/22/18/6816},
  selected={true},
  img_path={assets/img/sensors_review_overview.png},
  abstract={For decades, co-relating different data domains to attain the maximum potential of machines has driven research, especially in neural networks. Similarly, text and visual data (images and videos) are two distinct data domains with extensive research in the past. Recently, using natural language to process 2D or 3D images and videos with the immense power of neural nets has witnessed a promising future. Despite the diverse range of remarkable work in this field, notably in the past few years, rapid improvements have also solved future challenges for researchers. Moreover, the connection between these two domains is mainly subjected to GAN, thus limiting the horizons of this field. This review analyzes Text-to-Image (T2I) synthesis as a broader picture, Text-guided Visual-output (T2Vo), with the primary goal being to highlight the gaps by proposing a more comprehensive taxonomy. We broadly categorize text-guided visual output into three main divisions and meaningful subdivisions by critically examining an extensive body of literature from top-tier computer vision venues and closely related fields, such as machine learning and humanâ€“computer interaction, aiming at state-of-the-art models with a comparative analysis. This study successively follows previous surveys on T2I, adding value by analogously evaluating the diverse range of existing methods, including different generative models, several types of visual output, critical examination of various approaches, and highlighting the shortcomings, suggesting the future direction of research.}
}